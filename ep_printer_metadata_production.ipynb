{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e8ef7e",
   "metadata": {},
   "source": [
    "### Project Outline\n",
    "\n",
    "* Create 3 new columns: `namePreprocessed`, `_personID`, `nameRegularized`\n",
    "    * The leading underscore indicates `personId` is a temporary column. The ID can just be a sequential integer as long as it's unique - it doesn't need to be a true hash.\n",
    "    * Whether we can reliably create a `nameRegularized` - i.e. a modern regular version that should replace all variations in the metadata remains to be seen. It might not be possible without a lot of human intervention. \n",
    "* We need the following functions:\n",
    "    * `name_preprocess()`: Returns a cleaned up version of the name string or `None` if it doesn't look like a name.\n",
    "    * `substitution_cost()`: Needed for `weighted_levenshteain()`\n",
    "    * `substitution_cost_dict_generate()`: generate a cost dict for `weighted_levenshtein()`\n",
    "    * `weighted_levenshtein()`\n",
    "    * `name_pair()`: take a pair of preprocessed names and return a `true` if they are a close enough match that we should compute `weighted_levenshtein()` or `false` if we should ignore them. \n",
    "    * `name_pair_combinations()`: Calls name pair on all possible name pairs from the df\n",
    "    * `ner_pubStmt()`: Takes the `pubStmt()` field and runs NER with `Spacy` on it.\n",
    "* Procedure:\n",
    "    * Read data and create subset for ones that don't have VIAF ID. \n",
    "    * For each row, run `name_preprocess()`. If we get a name back, we store it in `namePreprocessed`\n",
    "        * The other rows get written out to a CSV. We need to clean up the pubStmt for these.\n",
    "    * Call the `name_pair_combinations()` function on the dataframe. \n",
    "    * This calls `name_pair()` on all combinations of names. We keep the viable name pairs.\n",
    "    * If they pass, we run them through `weighted_levenshtein()` and store the result in a `networkx` graph where the nodes are pd.Dataframe `id` and the edges are 1/weigthed_levenshtein() \\[i.e. the more similar the nodes, the higher the weight\\]. If the `weighted_levenshtein()` score is above a certain threshold, we don't add it to the graph. \n",
    "    * When we are done, we break down the graph into discrete subgraphs using [this approach](https://stackoverflow.com/questions/61536745/how-to-separate-an-unconnected-networkx-graph-into-multiple-mutually-disjoint-gr). Each subgraph will be one name in all its variant forms. This performs the clustering for us. \n",
    "    * We sort these graphs by number of nodes and assign each of them a unique ID starting at 1 and then write everythin out to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b549f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler\n",
    "\n",
    "# Set up all imports and logging\n",
    "import itertools, json, logging, re, string, sys\n",
    "from collections import defaultdict\n",
    "from statistics import multimode\n",
    "\n",
    "import networkx as nx\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from strsimpy.weighted_levenshtein import WeightedLevenshtein\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "sh = logging.StreamHandler(sys.stderr)\n",
    "sh.setLevel(logging.INFO)\n",
    "fmt = '%(asctime)s %(message)s'\n",
    "dfmt = '%y-%m-%d  %H:%M:%S'\n",
    "logging.basicConfig(handlers=(sh,), format=fmt, datefmt=dfmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb47d31",
   "metadata": {},
   "source": [
    "##### All adjustable parameters are set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data files to read\n",
    "printers_data_file = 'data/printers_etc.csv'\n",
    "name_abbreviations_file ='./data/name_abbreviations.json'\n",
    "\n",
    "# We'll output bad data to these files\n",
    "printers_data_file_date_notparsed_doubleyears = 'data/printers_etc_date_notparsed_doubleyears.csv'\n",
    "printers_data_file_date_notparsed = 'data/printers_etc_date_notparsed.csv'\n",
    "printers_data_file_pubstmt_notparsed = 'data/printers_etc_pubstmt_notparsed.csv'\n",
    "\n",
    "# Final output file\n",
    "printers_data_file_withhashes = 'data/printers_etc_withhashes.csv'\n",
    "\n",
    "n_test = 1000    # Subset of data for tests. Set to 0 or more than the total datasize to use everything.\n",
    "date_range = 10   # The span of years within which a printer is assumed to work\n",
    "records_with_viaf = False    # If False, we eliminate items that already have a resolved VIAF ID\n",
    "\n",
    "# Change these values to zoom in on shorter yearspans in the visualization\n",
    "start_year = 1400\n",
    "end_year = 1700\n",
    "records_within_datespan = True # If this is set, only the above datespan is kept\n",
    "\n",
    "# This needs to have both (c1, c2) and (c2, c1) pairs only if the weights are different. \n",
    "# Otherwise (c2, c1) etc is generated automatically below.\n",
    "substitution_cost_dict = {('j', 'i'): 0.2,\n",
    "                          ('u', 'v'): 0.2,\n",
    "                          ('i', 'y'): 0.2,\n",
    "                          ('e', 'y'):.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a631260",
   "metadata": {},
   "source": [
    "##### Read external files and visualize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name abbreviations dictionary can be passed to the name_preprocess function\n",
    "with open(name_abbreviations_file) as file:\n",
    "    name_abbreviations = json.load(file)\n",
    "\n",
    "printers_df = pd.read_csv(printers_data_file)\n",
    "\n",
    "# Convert dates to numeric but leave strings and nans untouched\n",
    "# Extract dates with two years - eg. 1660 1662\n",
    "printers_df['dateParsed'] = printers_df['dateParsed'].apply(pd.to_numeric, errors='ignore')\n",
    "printers_df_strings = printers_df[printers_df['dateParsed'].apply(lambda x: isinstance(x, str))]\n",
    "logging.info(f'Writing records with two date strings to {printers_data_file_date_notparsed_doubleyears}')\n",
    "printers_df_strings.to_csv(printers_data_file_date_notparsed_doubleyears)\n",
    "\n",
    "# Extract all rows with badly parsed dates (including above rows)\n",
    "printers_df['dateParsed'] = printers_df['dateParsed'].apply(pd.to_numeric, errors='coerce')\n",
    "filter_baddates = printers_df['dateParsed'].isna()\n",
    "printers_df_baddates = printers_df[filter_baddates]\n",
    "logging.info(f'Writing records with bad date fields to {printers_data_file_date_notparsed}')\n",
    "printers_df_baddates.to_csv(printers_data_file_date_notparsed)\n",
    "\n",
    "# Retain rows with well formed dates\n",
    "printers_df = printers_df[~filter_baddates]\n",
    "\n",
    "viaf_exists = printers_df[~printers_df['viafId'].isna()]\n",
    "viaf_needed = printers_df[printers_df['viafId'].isna()]\n",
    "\n",
    "total_counts = printers_df.groupby(['dateParsed'])['dateParsed'].count()\n",
    "viaf_exists_counts = viaf_exists.groupby(['dateParsed'])['dateParsed'].count()\n",
    "viaf_needed_counts = viaf_needed.groupby(['dateParsed'])['dateParsed'].count()\n",
    "\n",
    "# Set up the plot\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "grid = plt.GridSpec(3,1)\n",
    "axes = [fig.add_subplot(grid[0, 0]), fig.add_subplot(grid[1, 0]), fig.add_subplot(grid[2, 0])]\n",
    "\n",
    "sns.lineplot(data=total_counts, x=total_counts.index, y=total_counts.values, color='k', ax=axes[0])\n",
    "sns.lineplot(data=viaf_exists_counts, x=viaf_exists_counts.index, y=viaf_exists_counts.values, color='b', ax=axes[1])\n",
    "sns.lineplot(data=viaf_needed_counts, x=viaf_needed_counts.index, y=viaf_needed_counts.values, color='r', ax=axes[2])\n",
    "\n",
    "start_year = start_year if int(total_counts.index[0]) < start_year else int(total_counts.index[0])\n",
    "end_year = end_year if int(total_counts.index[-1]) > end_year else int(total_counts.index[-1])\n",
    "\n",
    "axes[0].set(xlim=(start_year, end_year), ylim=(1, max(total_counts[start_year:end_year].values+1)), xlabel=None, title='Total number of texts per year.')\n",
    "axes[1].set(xlim=(start_year, end_year), ylim=(0, max(total_counts[start_year:end_year].values)+1), xlabel=None, title='Number of texts per year with VIAF IDs.')\n",
    "axes[2].set(xlim=(start_year, end_year), ylim=(0, max(total_counts[start_year:end_year].values)+1), xlabel=None, title='Number of texts per year without VIAF IDs.')\n",
    "print('Total Number of texts:         {:,}\\nTexts with VIAF IDs:           {:,}\\nTexts without VIAF IDs:        {:,}\\nTexts with bad dates (ignored): {:,}'\\\n",
    "          .format(len(printers_df), len(viaf_exists), len(viaf_needed), len(printers_df_baddates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b298e17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# slice data to produce the final dataframe we'll work on\n",
    "if records_with_viaf:\n",
    "    df = printers_df\n",
    "    logging.info('Including records with assigned VIAF IDs')\n",
    "else:\n",
    "    df = viaf_needed\n",
    "    logging.info('Excluding records with assigned VIAF IDs')\n",
    "\n",
    "if records_within_datespan:\n",
    "    df = df[(df['dateParsed']>=start_year) & (df['dateParsed']<=end_year)]\n",
    "    logging.info(f'Keeping records within {start_year} and {end_year}')\n",
    "    \n",
    "if n_test==0:\n",
    "    df = df\n",
    "    logging.info(f'Keeping all {len(df)} records')\n",
    "else:\n",
    "    df = df[:n_test]\n",
    "    logging.info(f'Keeping the first {len(df)} records')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def name_preprocess(full_name, lower_case=False, title_case=True, \\\n",
    "                    name_abbreviations=name_abbreviations, \\\n",
    "                    max_length=30, min_tokens=2, max_tokens=3):\n",
    "    \"\"\"\n",
    "    Takes a single name string and returns a cleaned up version of the name or \n",
    "    None if it doesn't look like a name.\n",
    "    \n",
    "    We remove any preceding 'me', clean up punctuation, extra spaces. Throw out \n",
    "    names that are too long or have too few or too many tokens. Substitute vv -> w.\n",
    "    Also expand name abbreviations. Lower and Title case options available. Titlecase\n",
    "    takes precedence.\n",
    "    \"\"\"\n",
    "    # stop_words = ['de', 'the', 'of']\n",
    "    \n",
    "    full_name = unidecode.unidecode(full_name)\n",
    "    # If the string splits on a period, we likely have a lastname, firstname format\n",
    "    split_name = full_name.split(',')\n",
    "    if len(split_name)>1:\n",
    "        full_name = f'{split_name[1]} {split_name[0]}'\n",
    "\n",
    "    # Clean punctuation\n",
    "    full_name = re.sub(f'^me\\s*|[{string.punctuation}]|\\d*', '', full_name)\n",
    "    # Collapse vv to w\n",
    "    full_name = re.sub('vv', 'w', full_name)\n",
    "    full_name = re.sub('VV', 'W', full_name)\n",
    "\n",
    "    #ignore names that are too long\n",
    "    if len(full_name) > max_length:\n",
    "        logging.info(f'Too long: Ignoring: {full_name}')\n",
    "        return None\n",
    "\n",
    "    #find all strings separated by whitespace\n",
    "    words = re.findall(r'\\b\\w+\\b', full_name)\n",
    "    # If anything other than stopwords aren't capitalized we throw it away\n",
    "    # words = [w for w in words if (w[0].isupper() or w in stop_words)]\n",
    "    # check if the number of words is at least 2 or over 4\n",
    "    if len(words) < min_tokens:\n",
    "        logging.info(f'Too few tokens: Ignoring: {full_name}')\n",
    "        return None\n",
    "    if len(words) > max_tokens:\n",
    "        logging.info(f'Too many tokens: Ignoring: {full_name}')\n",
    "        return None\n",
    "    # return the first word and remaining string as a tuple\n",
    "    first_name = words[0]\n",
    "    if first_name in name_abbreviations:\n",
    "        first_name = name_abbreviations[first_name]\n",
    "    last_name = ' '.join(words[1:])\n",
    "    \n",
    "    if lower_case:\n",
    "        first_name = first_name.lower()\n",
    "        last_name = last_name.lower()\n",
    "    \n",
    "    if title_case:\n",
    "        first_name = first_name.title()\n",
    "        last_name = last_name.title()\n",
    "\n",
    "    return (first_name, last_name)\n",
    "\n",
    "def substitution_cost_dict_generate(substitution_cost_dict, swapcase_weight=0.2):\n",
    "    \"\"\"\n",
    "    Generate reverse pairs for the cost dictionary. I.e. is ('u', 'v') is supplied,\n",
    "    generate ('v', 'u'). ('u', 'V') and ('U', 'v') are also generated. Other letters \n",
    "    get the swapcase weight. So the cost of 'A'->'a' is swapcase_weight. This can be \n",
    "    set to zero, if you don't care about swapping cases.\n",
    "    \"\"\"\n",
    "    reversed_substitution_cost_dict = {}\n",
    "\n",
    "    for (c1, c2), w in substitution_cost_dict.items():\n",
    "        # (i, j) -> (j, i)\n",
    "        if (c2, c1) not in substitution_cost_dict:\n",
    "            reversed_substitution_cost_dict[(c2, c1)] = w\n",
    "        # (i, j) -> (i, J) and (I, j)\n",
    "        if (c1.swapcase(), c2) not in substitution_cost_dict:\n",
    "            reversed_substitution_cost_dict[(c1.swapcase(), c2)] = w\n",
    "        if (c1, c2.swapcase()) not in substitution_cost_dict:\n",
    "            reversed_substitution_cost_dict[(c1, c2.swapcase())] = w\n",
    "        if (c1.swapcase(), c2.swapcase()) not in substitution_cost_dict:\n",
    "            reversed_substitution_cost_dict[(c1.swapcase(), c2.swapcase())] = w\n",
    "        # (a, A), (A, a) etc\n",
    "        for l in string.ascii_letters:\n",
    "            if (l, l.swapcase()) not in substitution_cost_dict:\n",
    "                reversed_substitution_cost_dict[(l, l.swapcase())] = swapcase_weight\n",
    "\n",
    "    substitution_cost_dict = {**substitution_cost_dict, **reversed_substitution_cost_dict}\n",
    "\n",
    "    return substitution_cost_dict\n",
    "\n",
    "def substitution_cost(x, y, substitution_cost_dict=substitution_cost_dict):\n",
    "    \"\"\"\n",
    "    Takes a pair of letters and returns a substitution cost for them\n",
    "    \"\"\"\n",
    "    sc = substitution_cost_dict[(x,y)] if (x, y) in substitution_cost_dict else 1.0\n",
    "    return sc\n",
    "\n",
    "def name_pair(n1, n2, letter_similarity_threshold=.5, last_name_length_similarity_threshold=3):\n",
    "    \"\"\"\n",
    "    Takes two preprocessed names (tuples) and returns true is they are a viable match and \n",
    "    False is they are unlikely to be a match.\n",
    "\n",
    "    If names have same or similar initials. \n",
    "    If the last name is approximately same length.\n",
    "    \"\"\"\n",
    "    f1, l1 = n1\n",
    "    f2, l2 = n2\n",
    "    \n",
    "    # Do they have same or similar initials\n",
    "    if (((f1[0]==f2[0]) or (substitution_cost(f1[0], f2[0])<letter_similarity_threshold)) and \\\n",
    "                      ((l1[0]==l2[0]) or (substitution_cost(l1[0], l2[0])<letter_similarity_threshold))):\n",
    "        if abs(len(l1)-len(l2)) <= last_name_length_similarity_threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def name_pair_combinations(df, dateRange=40):\n",
    "    \"\"\"\n",
    "    Checks all names in a dataframe against each other and returns vialble name pairs \n",
    "    as a list of ((index, name), (index, name)) tuples. Note that each name will show\n",
    "    up at least once as it is similar to itself. So unique names are accounted for.\n",
    "    \"\"\"\n",
    "    filter = []\n",
    "    df = tuple(df.itertuples())\n",
    "    for i, n1 in enumerate(df):\n",
    "        if i%1000 == 0:\n",
    "            logging.info(f'Name pairs parsed: {i}')\n",
    "        for n2 in df[i:]:\n",
    "            if abs(n1[1] - n2[1]) <= dateRange:\n",
    "                name1 = n1[2]\n",
    "                name2 = n2[2]\n",
    "                index1 = n1[0]\n",
    "                index2 = n2[0]\n",
    "                if name_pair(name1, name2):\n",
    "                    filter.append(((index1, name1), (index2, name2)))\n",
    "    return filter\n",
    "\n",
    "def weighted_levenshtein_compute(n1, n2, weighted_levenshtein_function, initial_match_weight=.2):\n",
    "    f1, l1 = n1\n",
    "    f2, l2 = n2\n",
    "\n",
    "    if (f1 == f2) and (l1 == l2):\n",
    "        d1 = 0\n",
    "        d2 = 0\n",
    "    elif (len(f1) == 1 or len(f2) == 1) and (substitution_cost(f1[0], f2[0])<1):\n",
    "        d1 = initial_match_weight\n",
    "        d2 = weighted_levenshtein(l1, l2)\n",
    "    else:\n",
    "        d1 = weighted_levenshtein(f1, f2)\n",
    "        d2 = weighted_levenshtein(l1, l2)\n",
    "    return (d1+d2)\n",
    "  \n",
    "def weighted_levenshtein_compute_sortedlist(name_pairs, reverse=False):\n",
    "    distances = {}\n",
    "    for ((i1, n1), (i2, n2)) in name_pairs:\n",
    "        dist = weighted_levenshtein_compute(n1, n2, weighted_levenshtein)\n",
    "        distances[((i1, n1), (i2, n2))] = dist\n",
    "    distances = {k:v for k, v in sorted(distances.items(), key=lambda x: x[1], reverse=reverse)}\n",
    "    return distances\n",
    "    \n",
    "def dict_to_graph(distances, cut_off=3.5, min_dist=.1):\n",
    "    graph = nx.Graph()\n",
    "    i = 0\n",
    "    for k in distances: \n",
    "        i+=1\n",
    "        if distances[k] <= cut_off:\n",
    "            graph.add_edge(k[0], k[1], weight=1/(distances[k]+min_dist))\n",
    "    return(graph)     \n",
    "        \n",
    "# We'll call the above function to expand our cost dictionary\n",
    "substitution_cost_dict = substitution_cost_dict_generate(substitution_cost_dict)\n",
    "\n",
    "# Initialize the WL function with custom weights\n",
    "weighted_levenshtein = WeightedLevenshtein(substitution_cost_fn=substitution_cost).distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85236475",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Main body\n",
    "# Preprocess all names and create new column for it\n",
    "df['namePreprocessed'] = df['name'].map(name_preprocess)\n",
    "# Separate rows that don't look like names. Write them out to file for cleaning\n",
    "df_unresolved_name = df[df['namePreprocessed'].isnull()]\n",
    "logging.info(f'Writing {len(df_unresolved_name)} unresolved names to {printers_data_file_pubstmt_notparsed}')\n",
    "df_unresolved_name.to_csv(printers_data_file_pubstmt_notparsed)\n",
    "\n",
    "# Keep only resolved names\n",
    "df = df[~df['namePreprocessed'].isnull()]\n",
    "logging.info(f'{len(df)} names resolved')\n",
    "\n",
    "# Trim the dataframe - keep only nameProcessed and parsedDate\n",
    "df_trimmed = df[['dateParsed', 'namePreprocessed']]\n",
    "\n",
    "# Generate list of viable name pairs\n",
    "name_pairs = name_pair_combinations(df_trimmed, dateRange=date_range)\n",
    "# Compute sorted list of levenshtein distances\n",
    "distances = weighted_levenshtein_compute_sortedlist(name_pairs, reverse=False)\n",
    "\n",
    "# Convert dictionary to a graph and break into a sorted list of subgraphs\n",
    "g = dict_to_graph(distances)\n",
    "sub_g = sorted(nx.connected_components(g), key=len, reverse=True)\n",
    "indices = [[i for (i, n) in sg]for sg in sub_g]\n",
    "names = [[n for (i, n) in sg]for sg in sub_g]\n",
    "for hash_ind, sub_graph in enumerate(indices):\n",
    "    name_pool = names[hash_ind]\n",
    "    first_names, last_names = zip(*name_pool)\n",
    "    name = f'{multimode(first_names)[0]} {multimode(last_names)[0]}'\n",
    "    for row in sub_graph:\n",
    "        df.at[row,'indexHash'] = hash_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6845bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['namePreprocessed'], axis=1)\n",
    "df.to_csv(printers_data_file_withhashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Slower implementatitions of above function\n",
    "# def name_pair_combinations_(df, dateRange=40):\n",
    "#     \"\"\"\n",
    "#     Returns vialble name pairs as a list of indices. Note that each name will show\n",
    "#     up at least once as it is similar to itself. So unique names are accounted for.\n",
    "#     \"\"\"\n",
    "#     filter = []\n",
    "#     for i, ni1 in enumerate(df.itertuples()):\n",
    "#         if i%1000 == 0:\n",
    "#             logging.info(f'Name pairs parsed: {i}')\n",
    "#         for ni2 in df[i:].itertuples():\n",
    "#             if abs(ni1.dateParsed - ni2.dateParsed) <= dateRange:\n",
    "#                 name1 = ni1.namePreprocessed\n",
    "#                 name2 = ni2.namePreprocessed\n",
    "#                 index1 = ni1.Index\n",
    "#                 index2 = ni2.Index\n",
    "#                 if name_pair(name1, name2):\n",
    "#                     filter.append(((index1, name1), (index2, name2)))\n",
    "#     return filter\n",
    "\n",
    "# def name_pair_combinations__(df):\n",
    "#     df_trimmed = df.drop(['tcpid','role','role_edited','name','source','title','author','date', 'parsedDate','place','pubStmt','nameResolved','viafId'], axis=1)\n",
    "#     df_trimmed['indexCopy'] = df_trimmed.index\n",
    "#     dtj = df_trimmed.join(df_trimmed, how='cross', lsuffix='_1', rsuffix='_2')\n",
    "#     return dtj.apply(lambda x: name_pair(x['namePreprocessed_1'], x['namePreprocessed_2']), axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
